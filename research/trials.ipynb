{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "aad038c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root: /Users/lathifmohammadshaik/Desktop/sutent gpt/Buliding-a-complete-subject-chatbot-with-LLMs-LangChain-Pinecone-Flask-AWS\n",
      "Keys in .env: ['PINECONE_API_KEY', 'OPENROUTER_API_KEY']\n",
      "Has PINECONE_API_KEY? True\n",
      "Has OPENROUTER_API_KEY? True\n"
     ]
    }
   ],
   "source": [
    "import os, pathlib\n",
    "from dotenv import load_dotenv, dotenv_values\n",
    "\n",
    "# move up from /research → project root\n",
    "if pathlib.Path.cwd().name == \"research\":\n",
    "    os.chdir(\"..\")\n",
    "\n",
    "# load keys from .env (override any old values)\n",
    "load_dotenv(override=True)\n",
    "\n",
    "# sanity print\n",
    "vals = dotenv_values(\".env\")\n",
    "print(\"Project root:\", pathlib.Path.cwd())\n",
    "print(\"Keys in .env:\", list(vals.keys()))\n",
    "print(\"Has PINECONE_API_KEY?\", bool(os.getenv(\"PINECONE_API_KEY\")))\n",
    "print(\"Has OPENROUTER_API_KEY?\", bool(os.getenv(\"OPENROUTER_API_KEY\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ab381b99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 834 pages\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import DirectoryLoader, PyPDFLoader\n",
    "\n",
    "def load_pdf_files(data_dir=\"data\"):\n",
    "    loader = DirectoryLoader(data_dir, glob=\"*.pdf\", loader_cls=PyPDFLoader)\n",
    "    docs = loader.load()\n",
    "    print(f\"Loaded {len(docs)} pages\")\n",
    "    return docs\n",
    "\n",
    "raw_docs = load_pdf_files(\"data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b5b75a38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample source: {'source': 'data/CyBOK-version-1.0.pdf'}\n"
     ]
    }
   ],
   "source": [
    "from typing import List\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "def to_minimal_docs(docs: List[Document]) -> List[Document]:\n",
    "    out = []\n",
    "    for d in docs:\n",
    "        meta = d.metadata or {}\n",
    "        src = meta.get(\"source\") or meta.get(\"file_path\") or \"unknown\"\n",
    "        out.append(Document(page_content=d.page_content, metadata={\"source\": src}))\n",
    "    return out\n",
    "\n",
    "minimal_docs = to_minimal_docs(raw_docs)\n",
    "print(\"Sample source:\", minimal_docs[0].metadata if minimal_docs else {})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "09cfb715",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split completed: 7178 chunks created.\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500,\n",
    "    chunk_overlap=20,\n",
    "    length_function=len\n",
    ")\n",
    "texts_chunk = splitter.split_documents(minimal_docs)\n",
    "print(f\"Split completed: {len(texts_chunk)} chunks created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cb6433ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/n_/9cmjbb9s17q08sntmjt5hdcr0000gn/T/ipykernel_11749/2212488852.py:4: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the `langchain-huggingface package and should be used instead. To use it run `pip install -U `langchain-huggingface` and import as `from `langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings = HuggingFaceEmbeddings(model_name=EMBED_MODEL)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector length: 384\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "EMBED_MODEL = \"sentence-transformers/all-MiniLM-L6-v2\"  # 384-dim\n",
    "embeddings = HuggingFaceEmbeddings(model_name=EMBED_MODEL)\n",
    "\n",
    "# quick smoke test\n",
    "vec = embeddings.embed_query(\"hello world\")\n",
    "print(\"Vector length:\", len(vec))  # should be 384"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "17b50f09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Using existing Pinecone index: subject-chatbot\n",
      "✅ Connected to index: subject-chatbot\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "\n",
    "if not os.getenv(\"PINECONE_API_KEY\"):\n",
    "    raise RuntimeError(\"No PINECONE_API_KEY in env.\")\n",
    "\n",
    "pc = Pinecone(api_key=os.environ[\"PINECONE_API_KEY\"])\n",
    "\n",
    "INDEX_NAME = \"subject-chatbot\"\n",
    "DIMENSION  = 384\n",
    "METRIC     = \"cosine\"\n",
    "\n",
    "if not pc.has_index(INDEX_NAME):\n",
    "    pc.create_index(\n",
    "        name=INDEX_NAME,\n",
    "        dimension=DIMENSION,\n",
    "        metric=METRIC,\n",
    "        spec=ServerlessSpec(cloud=\"aws\", region=\"us-east-1\")\n",
    "    )\n",
    "    print(\"✅ Created Pinecone index:\", INDEX_NAME)\n",
    "else:\n",
    "    print(\"✅ Using existing Pinecone index:\", INDEX_NAME)\n",
    "\n",
    "index = pc.Index(INDEX_NAME)\n",
    "print(\"✅ Connected to index:\", INDEX_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0be81147",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Has Pinecone key: True\n",
      "Has Groq key: True\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv, dotenv_values\n",
    "\n",
    "load_dotenv(override=True)\n",
    "vals = dotenv_values(\".env\")\n",
    "print(\"Has Pinecone key:\", bool(os.getenv(\"PINECONE_API_KEY\")))\n",
    "print(\"Has Groq key:\", bool(os.getenv(\"GROQ_API_KEY\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "87bccaab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 834 pages\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import DirectoryLoader, PyPDFLoader\n",
    "\n",
    "def load_pdf_files(data_dir=\"data\"):\n",
    "    loader = DirectoryLoader(\n",
    "        data_dir,\n",
    "        glob=\"*.pdf\",\n",
    "        loader_cls=PyPDFLoader\n",
    "    )\n",
    "    docs = loader.load()\n",
    "    print(f\"Loaded {len(docs)} pages\")\n",
    "    return docs\n",
    "\n",
    "raw_docs = load_pdf_files()\n",
    "assert raw_docs, \"No PDFs found in ./data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8007145e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimized: 834\n"
     ]
    }
   ],
   "source": [
    "from typing import List\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "def to_minimal_docs(docs: List[Document]) -> List[Document]:\n",
    "    out = []\n",
    "    for d in docs:\n",
    "        meta = d.metadata or {}\n",
    "        src  = meta.get(\"source\") or meta.get(\"file_path\") or \"unknown\"\n",
    "        out.append(Document(page_content=d.page_content, metadata={\"source\": src}))\n",
    "    print(\"Minimized:\", len(out))\n",
    "    return out\n",
    "\n",
    "minimal_docs = to_minimal_docs(raw_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4a7eaac8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimized: 834\n"
     ]
    }
   ],
   "source": [
    "from typing import List\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "def to_minimal_docs(docs: List[Document]) -> List[Document]:\n",
    "    out = []\n",
    "    for d in docs:\n",
    "        meta = d.metadata or {}\n",
    "        src  = meta.get(\"source\") or meta.get(\"file_path\") or \"unknown\"\n",
    "        out.append(Document(page_content=d.page_content, metadata={\"source\": src}))\n",
    "    print(\"Minimized:\", len(out))\n",
    "    return out\n",
    "\n",
    "minimal_docs = to_minimal_docs(raw_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "68933d82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunks: 7178\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=20)\n",
    "texts_chunk = splitter.split_documents(minimal_docs)\n",
    "print(\"Chunks:\", len(texts_chunk))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d9b954d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector length: 384\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "EMBED_MODEL = \"sentence-transformers/all-MiniLM-L6-v2\"  # 384-dim\n",
    "embeddings = HuggingFaceEmbeddings(model_name=EMBED_MODEL)\n",
    "\n",
    "# quick smoke test\n",
    "vec = embeddings.embed_query(\"hello world\")\n",
    "print(\"Vector length:\", len(vec))  # should be 384"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5a3cd29d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upserted 7178 chunks into index: subject-chatbot\n"
     ]
    }
   ],
   "source": [
    "from langchain_pinecone import PineconeVectorStore\n",
    "\n",
    "INDEX_NAME = \"subject-chatbot\"  # same name you created\n",
    "vectorstore = PineconeVectorStore.from_documents(\n",
    "    documents=texts_chunk,\n",
    "    embedding=embeddings,\n",
    "    index_name=INDEX_NAME\n",
    ")\n",
    "print(f\"Upserted {len(texts_chunk)} chunks into index: {INDEX_NAME}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "87a7714a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved 3 chunks.\n",
      " 1. source=data/CyBOK-version-1.0.pdf | preview=The table below lists the reference material that serves as the basis for for this chapter and explains how it relates t…\n",
      " 2. source=../data/CyBOK-version-1.0.pdf | preview=The table below lists the reference material that serves as the basis for for this chapter and explains how it relates t…\n",
      " 3. source=../data/CyBOK-version-1.0.pdf | preview=much of this knowledge area’s content will be novel to those whose education is based in sci- ence, technology, engineer…\n"
     ]
    }
   ],
   "source": [
    "retriever = vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 3})\n",
    "\n",
    "query = \"Give a short overview of the document's main subject.\"\n",
    "docs = retriever.invoke(query)  # .invoke in LC v0.2+\n",
    "print(f\"Retrieved {len(docs)} chunks.\")\n",
    "for i, d in enumerate(docs, 1):\n",
    "    src = d.metadata.get(\"source\", \"unknown\")\n",
    "    preview = d.page_content[:120].replace(\"\\n\", \" \")\n",
    "    print(f\"{i:>2}. source={src} | preview={preview}…\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bb4bfa93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenRouter DeepSeek LLM ready.\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "import os\n",
    "\n",
    "# --- OpenRouter DeepSeek model ---\n",
    "if not os.getenv(\"OPENROUTER_API_KEY\"):\n",
    "    raise RuntimeError(\"No OPENROUTER_API_KEY in env. Add it to your .env file.\")\n",
    "\n",
    "chat_llm = ChatOpenAI(\n",
    "    model=\"tngtech/deepseek-r1t2-chimera:free\",   # OpenRouter model slug\n",
    "    base_url=\"https://openrouter.ai/api/v1\",      # Required for OpenRouter\n",
    "    api_key=os.getenv(\"OPENROUTER_API_KEY\"),\n",
    "    temperature=0\n",
    ")\n",
    "\n",
    "print(\"OpenRouter DeepSeek LLM ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a96e99b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ RAG chain ready (v1.x).\n"
     ]
    }
   ],
   "source": [
    "# LangChain v1.x RAG (works with 1.0.5)\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# 1) Build a retriever from your existing vectorstore\n",
    "retriever = vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 3})\n",
    "\n",
    "# 2) Helper: format retrieved docs into a single context string\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(d.page_content for d in docs)\n",
    "\n",
    "# 3) Prompt\n",
    "system_prompt = (\n",
    "    \"You are a concise QA assistant.\\n\"\n",
    "    \"Use the provided context to answer the user's question.\\n\"\n",
    "    \"If you don't know, say you don't know. Keep answers short.\\n\\n\"\n",
    "    \"{context}\"\n",
    ")\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", system_prompt),\n",
    "    (\"human\", \"{input}\")\n",
    "])\n",
    "\n",
    "# 4) Compose the RAG pipeline\n",
    "#    input -> {\"context\": retriever(input) -> format_docs, \"input\": passthrough} -> prompt -> LLM -> text\n",
    "rag_chain = (\n",
    "    {\n",
    "        \"context\": retriever | format_docs,\n",
    "        \"input\": RunnablePassthrough(),\n",
    "    }\n",
    "    | prompt\n",
    "    | chat_llm         # you already set this earlier (OpenAI or Groq)\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "print(\"✅ RAG chain ready (v1.x).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0fb37830",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "— ANSWER —\n",
      " \n",
      "The document's main subject is **Physical Layer Security and Telecommunications**. It serves as a reference guide for a chapter, organizing material by topics and sub-topics to explain their relevance.\n"
     ]
    }
   ],
   "source": [
    "# ✅ Correct test call for LangChain v1.x RAG\n",
    "question = \"Give a short overview of the document’s main subject.\"\n",
    "answer = rag_chain.invoke(question)\n",
    "\n",
    "print(\"— ANSWER —\\n\", answer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "subjectbot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
